{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self Normalizing Networks (SNN) w/ Scaled Exponential Linear Units (SELU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1. Gradient issues\n",
    "    2. Normalization/Standardization\n",
    "    3. Weight Normalization\n",
    "    4. Batch Normalization\n",
    "    5. Layer Normalization\n",
    "    6. Self Normalizing Networks (SNN)\n",
    "    7. Scaled Exponential Linear Units (SELU)\n",
    "    8. Implementation\n",
    "    9. Additional Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "\n",
    "from pycrayon import (\n",
    "    CrayonClient,\n",
    ")\n",
    "\n",
    "from IPython import (\n",
    "    display,\n",
    ")\n",
    "\n",
    "from IPython.display import (\n",
    "    Image,\n",
    "    clear_output,\n",
    ")\n",
    "\n",
    "# get matplotlib configuration\n",
    "%matplotlib inline\n",
    "%run plot_conf.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.autograd import (\n",
    "    Variable,\n",
    ")\n",
    "\n",
    "from torch.nn import (\n",
    "    init,\n",
    ")\n",
    "\n",
    "from torchvision import (\n",
    "    datasets, \n",
    "    transforms,\n",
    "    models,\n",
    "    utils,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding MATERIALS for NLPWP (may include selu stuff with it)\n",
    "No GPU used b/c of book standards and can't access it one this Key..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "Self-Normalizing Neural Networks\n",
    "GÃ¼nter Klambauer, Thomas Unterthiner, Andreas Mayr, Sepp Hochreiter\n",
    "https://arxiv.org/abs/1706.02515\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Caution:** Not an official implementation. There going to be a lot of overfitting, etc. Just wanted to see how SELU works but MNIST isn't giving great insight ... I'll explore some more interesting tasks and use deeper models later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train data loaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data/mnist', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Test data loader\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data/mnist', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA5sAAADGCAYAAABck5SkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDZH6P4cAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d61c400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize some samples\n",
    "for batch in test_loader:\n",
    "    samples = batch[0][:5]\n",
    "    y_true = batch[1]\n",
    "    plt.subplot(1, 5, 1)\n",
    "    for i, sample in enumerate(samples):\n",
    "        plt.subplot(1, 5, i+1)\n",
    "        plt.title('Number: %i'% y_true[i])\n",
    "        plt.imshow(sample.numpy().reshape((28,28)))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_batch(inputs, targets, model, criterion, optimizer, cuda, is_training):\n",
    "    \"\"\"\n",
    "    Process a minibatch for loss and accuracy.\n",
    "    \"\"\"\n",
    "    \n",
    "    if cuda:\n",
    "        inputs, target = inputs.cuda(), target.cuda()\n",
    "    \n",
    "    # Convert tensors to Variables (for autograd)\n",
    "    if is_training:\n",
    "        X_batch = Variable(inputs, requires_grad=False)\n",
    "    else:\n",
    "        X_batch = Variable(inputs, volatile=True, requires_grad=False)\n",
    "    y_batch = Variable(targets.long(), requires_grad=False)\n",
    "    \n",
    "    # Forward pass\n",
    "    activations, scores = model(X_batch) # logits\n",
    "\n",
    "    # Loss\n",
    "    loss = criterion(scores, y_batch)\n",
    "    \n",
    "    # Accuracy\n",
    "    score, predicted = torch.max(scores, 1)\n",
    "    accuracy = (y_batch.data == predicted.data).sum() / float(len(y_batch))\n",
    "    \n",
    "    if is_training:\n",
    "\n",
    "        # Use autograd to do backprop. This will compute the\n",
    "        # gradients w.r.t loss for all Variables that have\n",
    "        # requires_grad=True. So, our w1 and w2 will now have\n",
    "        # gradient components we can access.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip the gradient norms\n",
    "        nn.utils.clip_grad_norm(model.parameters(), max_grad_norm)\n",
    "\n",
    "        # Update params\n",
    "        optimizer.step()\n",
    "\n",
    "    return loss, accuracy, activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(exp, exp_name, model, criterion, optimizer, train_loader, test_loader, \n",
    "          num_epochs, batch_size, log_interval, learning_rate,\n",
    "          dropout_p, decay_rate, max_grad_norm, cuda):\n",
    "    \"\"\"\n",
    "    Training.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Metrics\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    test_loss = []\n",
    "    test_acc = []\n",
    "\n",
    "    # Training\n",
    "    for num_train_epoch in range(num_epochs):\n",
    "\n",
    "        # Timer\n",
    "        start = time.time()\n",
    "\n",
    "        # Decay learning rate\n",
    "        learning_rate = learning_rate * (decay_rate ** (num_train_epoch // 1.0))\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = learning_rate\n",
    "\n",
    "        # Metrics\n",
    "        train_batch_loss = 0.0\n",
    "        train_batch_accuracy = 0.0\n",
    "\n",
    "        for train_batch_num, (inputs, target) in enumerate(train_loader):\n",
    "           \n",
    "            if exp_name.split(\"_\")[-1] == \"mlp\":\n",
    "                # Flatten inputs to feed into MLP\n",
    "                inputs = inputs.view(len(inputs), -1)\n",
    "\n",
    "            # Get metrics\n",
    "            model.train()\n",
    "            loss, accuracy, activations = process_batch(inputs, target, model, criterion, optimizer, cuda, model.training)\n",
    "\n",
    "            # Add to batch scalars\n",
    "            train_batch_loss += loss.data[0] / float(len(inputs))\n",
    "            train_batch_accuracy += accuracy\n",
    "                        \n",
    "            # Collect activations\n",
    "            for activation_num, activation in enumerate(activations):\n",
    "                exp.add_histogram_value(\n",
    "                    \"z%i\"%(activation_num+1), hist=list(activation.data.view(-1, )), tobuild=True)\n",
    "            \n",
    "            # Record metrics\n",
    "            exp.add_scalar_value(\"train_loss\", value=loss.data[0] / float(len(inputs)))\n",
    "            exp.add_scalar_value(\"train_accuracy\", value=accuracy)\n",
    "            \n",
    "        # Add to global metrics\n",
    "        train_loss.append(train_batch_loss/float(train_batch_num+1))\n",
    "        train_acc.append(train_batch_accuracy/float(train_batch_num+1))\n",
    "\n",
    "        # Testing\n",
    "        model.eval()\n",
    "        for num_test_epoch in range(1):\n",
    "\n",
    "            # Metrics\n",
    "            test_batch_loss = 0.0\n",
    "            test_batch_accuracy = 0.0\n",
    "\n",
    "            for test_batch_num, (inputs, target) in enumerate(test_loader):\n",
    "                \n",
    "                if exp_name.split(\"_\")[-1] == \"mlp\":\n",
    "                    # Flatten inputs to feed into MLP\n",
    "                    inputs = inputs.view(len(inputs), -1)\n",
    "\n",
    "                # Get metrics\n",
    "                model.eval()\n",
    "                loss, accuracy, activations = \\\n",
    "                    process_batch(inputs, target, model, criterion, optimizer, cuda, model.training)\n",
    "\n",
    "                # Add to batch scalars\n",
    "                test_batch_loss += loss.data[0] / float(len(inputs))\n",
    "                test_batch_accuracy += accuracy\n",
    "                \n",
    "                # Record metrics\n",
    "                exp.add_scalar_value(\"test_loss\", value=loss.data[0] / float(len(inputs)))\n",
    "                exp.add_scalar_value(\"test_accuracy\", value=accuracy)\n",
    "\n",
    "            # Add to global metrics\n",
    "            test_loss.append(test_batch_loss/float(test_batch_num+1))\n",
    "            test_acc.append(test_batch_accuracy/float(test_batch_num+1))\n",
    "                \n",
    "\n",
    "            verbose_condition = \\\n",
    "                (num_train_epoch == 0) or (num_train_epoch % log_interval == 0) or (num_train_epoch == num_epochs-1)\n",
    "\n",
    "            # Verbose\n",
    "            if verbose_condition:\n",
    "\n",
    "                # Verbose\n",
    "                time_remain = (time.time() - start) * (num_epochs - (num_train_epoch+1))\n",
    "                minutes = time_remain // 60\n",
    "                seconds = time_remain - minutes*60\n",
    "                print(\"TIME REMAINING: %im %is\" % (minutes, seconds))\n",
    "                print(\"[EPOCH]: %i, [TRAIN LOSS]: %.6f, [TRAIN ACC]: %.3f, [TEST LOSS]: %.6f, [TEST ACC]: %.3f\" %\n",
    "                       (num_train_epoch, train_batch_loss/float(train_batch_num+1), \n",
    "                        train_batch_accuracy/float(train_batch_num+1), test_batch_loss/float(test_batch_num+1),\n",
    "                        test_batch_accuracy/float(test_batch_num+1)))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up Crayon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Connect to the server for Crayon (tensorboard)\n",
    "cc = CrayonClient(hostname=\"localhost\", port=8889)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training params\n",
    "num_epochs = 3\n",
    "learning_rate = 1e-3\n",
    "dropout_p = 0.05\n",
    "log_interval = 1 # epochs\n",
    "num_hidden_units = 100\n",
    "num_classes = 10 # MNIST\n",
    "decay_rate = 0.9999\n",
    "max_grad_norm = 5.0\n",
    "\n",
    "# Use gpu\n",
    "cuda = True\n",
    "cuda = cuda and torch.cuda.is_available()\n",
    "\n",
    "# reproduceability\n",
    "seed = 1234\n",
    "torch.manual_seed(seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReLU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create relu experiment\n",
    "exp_name = \"relu_mlp\"\n",
    "try:\n",
    "    cc.remove_experiment(exp_name)\n",
    "    exp = cc.create_experiment(exp_name)\n",
    "except:\n",
    "    exp = cc.create_experiment(exp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class relu_mlp(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple MLP with ReLU non-linearities.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_hidden_units, num_classes):\n",
    "        \"\"\"\n",
    "        Initialization.\n",
    "        \"\"\"\n",
    "        super(relu_mlp, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, num_hidden_units)\n",
    "        self.fc2 = nn.Linear(num_hidden_units, num_hidden_units)\n",
    "        self.fc3 = nn.Linear(num_hidden_units, num_classes)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Properly initialize weights.\n",
    "        \"\"\"\n",
    "        init.xavier_uniform(self.fc1.weight, gain=np.sqrt(2.0)) # gain for ReLU\n",
    "        init.xavier_uniform(self.fc2.weight, gain=np.sqrt(2.0)) # gain for ReLU\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Feed forward.\n",
    "        \"\"\"\n",
    "        z1 = F.relu(self.fc1(x))\n",
    "        z2 = F.relu(self.fc2(z1))\n",
    "        z3 = self.fc3(z2)\n",
    "        return [z1, z2, z3], z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME REMAINING: 1m 35s\n",
      "[EPOCH]: 0, [TRAIN LOSS]: 0.002720, [TRAIN ACC]: 0.919, [TEST LOSS]: 0.001336, [TEST ACC]: 0.958\n",
      "TIME REMAINING: 0m 46s\n",
      "[EPOCH]: 1, [TRAIN LOSS]: 0.001132, [TRAIN ACC]: 0.966, [TEST LOSS]: 0.001081, [TEST ACC]: 0.967\n",
      "TIME REMAINING: 0m 0s\n",
      "[EPOCH]: 2, [TRAIN LOSS]: 0.000781, [TRAIN ACC]: 0.976, [TEST LOSS]: 0.000921, [TEST ACC]: 0.972\n"
     ]
    }
   ],
   "source": [
    "# Initialize model components\n",
    "model = relu_mlp(num_hidden_units, num_classes)\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "model = train(exp, exp_name, model, criterion, optimizer, train_loader, test_loader, \n",
    "          num_epochs, batch_size, log_interval, learning_rate,\n",
    "          dropout_p, decay_rate, max_grad_norm, cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SELU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create relu experiment\n",
    "exp_name = \"selu_mlp\"\n",
    "try:\n",
    "    cc.remove_experiment(exp_name)\n",
    "    exp = cc.create_experiment(exp_name)\n",
    "except:\n",
    "    exp = cc.create_experiment(exp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def selu(x):\n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    scale = 1.0507009873554804934193349852946\n",
    "    return scale * F.elu(x, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class selu_mlp(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple MLP with ReLU non-linearities.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_hidden_units, num_classes):\n",
    "        \"\"\"\n",
    "        Initialization.\n",
    "        \"\"\"\n",
    "        super(selu_mlp, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, num_hidden_units)\n",
    "        self.fc2 = nn.Linear(num_hidden_units, num_hidden_units)\n",
    "        self.fc3 = nn.Linear(num_hidden_units, num_classes)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Properly initialize weights from a gaussian distribution w/ mean=0 and var=1/n\n",
    "        \"\"\"\n",
    "        # weights\n",
    "        self.fc1.weight.data.normal_(mean=0, std=np.sqrt(1/len(self.fc1.weight.data[0])))\n",
    "        self.fc2.weight.data.normal_(mean=0, std=np.sqrt(1/len(self.fc2.weight.data[0])))\n",
    "        self.fc3.weight.data.normal_(mean=0, std=np.sqrt(1/len(self.fc3.weight.data[0])))\n",
    "        \n",
    "        # biases\n",
    "        self.fc1.bias.data.normal_(mean=0, std=0.1)\n",
    "        self.fc2.bias.data.normal_(mean=0, std=0.1)\n",
    "        self.fc3.bias.data.normal_(mean=0, std=0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Feed forward.\n",
    "        \"\"\"\n",
    "        z1 = selu(self.fc1(x))\n",
    "        z2 = selu(self.fc2(z1))\n",
    "        z3 = self.fc3(z2)\n",
    "        return [z1, z2, z3], z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME REMAINING: 1m 52s\n",
      "[EPOCH]: 0, [TRAIN LOSS]: 0.002625, [TRAIN ACC]: 0.920, [TEST LOSS]: 0.001453, [TEST ACC]: 0.957\n",
      "TIME REMAINING: 0m 55s\n",
      "[EPOCH]: 1, [TRAIN LOSS]: 0.001183, [TRAIN ACC]: 0.964, [TEST LOSS]: 0.001094, [TEST ACC]: 0.965\n",
      "TIME REMAINING: 0m 0s\n",
      "[EPOCH]: 2, [TRAIN LOSS]: 0.000826, [TRAIN ACC]: 0.975, [TEST LOSS]: 0.000930, [TEST ACC]: 0.970\n"
     ]
    }
   ],
   "source": [
    "# Initialize model components\n",
    "model = selu_mlp(num_hidden_units, num_classes)\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "model = train(exp, exp_name, model, criterion, optimizer, train_loader, test_loader, \n",
    "          num_epochs, batch_size, log_interval, learning_rate,\n",
    "          dropout_p, decay_rate, max_grad_norm, cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/key_mlp.png\"></img>\n",
    "<img src=\"figures/activations_mlp.png\"></img>\n",
    "<img src=\"figures/train_acc_mlp.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training params\n",
    "num_epochs = 3\n",
    "learning_rate = 0.01\n",
    "dropout_p = 0.5\n",
    "log_interval = 1 # epochs\n",
    "num_hidden_units = 100\n",
    "num_classes = 10 # MNIST\n",
    "decay_rate = 0.9999\n",
    "max_grad_norm = 5.0\n",
    "\n",
    "# Use gpu\n",
    "cuda = True\n",
    "cuda = cuda and torch.cuda.is_available()\n",
    "\n",
    "# reproduceability\n",
    "seed = 1234\n",
    "torch.manual_seed(seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReLU CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create relu experiment\n",
    "exp_name = \"relu_cnn\"\n",
    "try:\n",
    "    cc.remove_experiment(exp_name)\n",
    "    exp = cc.create_experiment(exp_name)\n",
    "except:\n",
    "    exp = cc.create_experiment(exp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CNN model\n",
    "class relu_cnn(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple CNN for MNIST classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout_p, num_hidden_units, num_classes):\n",
    "        \"\"\"\n",
    "        Initialization.\n",
    "        \"\"\"\n",
    "        super(relu_cnn, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.dropout_conv2 = nn.Dropout2d(dropout_p)\n",
    "        self.fc1 = nn.Linear(320, num_hidden_units)\n",
    "        self.fc2 = nn.Linear(num_hidden_units, num_classes)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Properly initialize weights.\n",
    "        \"\"\"\n",
    "        init.xavier_uniform(self.conv1.weight, gain=np.sqrt(2.0)) # gain for ReLU\n",
    "        init.xavier_uniform(self.conv2.weight, gain=np.sqrt(2.0)) # gain for ReLU\n",
    "        init.xavier_uniform(self.fc1.weight, gain=np.sqrt(2.0)) # gain for ReLU\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Feed Foward.\n",
    "        \"\"\"\n",
    "        z1 = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        z2 = F.relu(F.max_pool2d(self.dropout_conv2(self.conv2(z1)), 2))\n",
    "        z2 = z2.view(-1, 320) # flatten\n",
    "        z3 = F.relu(self.fc1(z2))\n",
    "        z4 = F.dropout(z3, training=self.training)\n",
    "        z5 = self.fc2(z4)\n",
    "        return [z1, z2, z3, z4, z5], z5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize model components\n",
    "model = relu_cnn(dropout_p, num_hidden_units, num_classes)\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME REMAINING: 7m 38s\n",
      "[EPOCH]: 0, [TRAIN LOSS]: 0.007336, [TRAIN ACC]: 0.758, [TEST LOSS]: 0.001445, [TEST ACC]: 0.955\n",
      "TIME REMAINING: 3m 31s\n",
      "[EPOCH]: 1, [TRAIN LOSS]: 0.004509, [TRAIN ACC]: 0.862, [TEST LOSS]: 0.001250, [TEST ACC]: 0.963\n",
      "TIME REMAINING: 0m 0s\n",
      "[EPOCH]: 2, [TRAIN LOSS]: 0.004265, [TRAIN ACC]: 0.873, [TEST LOSS]: 0.001448, [TEST ACC]: 0.959\n"
     ]
    }
   ],
   "source": [
    "model = train(exp, exp_name, model, criterion, optimizer, train_loader, test_loader, \n",
    "          num_epochs, batch_size, log_interval, learning_rate,\n",
    "          dropout_p, decay_rate, max_grad_norm, cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### SELU CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dropout_p = 0.0 # keep 0 until implemented alpha dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create relu experiment\n",
    "exp_name = \"selu_cnn\"\n",
    "try:\n",
    "    cc.remove_experiment(exp_name)\n",
    "    exp = cc.create_experiment(exp_name)\n",
    "except:\n",
    "    exp = cc.create_experiment(exp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CNN model\n",
    "class selu_cnn(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple CNN for MNIST classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout_p, num_hidden_units, num_classes):\n",
    "        \"\"\"\n",
    "        Initialization.\n",
    "        \"\"\"\n",
    "        super(selu_cnn, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.dropout_conv2 = nn.Dropout2d(dropout_p)\n",
    "        self.fc1 = nn.Linear(320, num_hidden_units)\n",
    "        self.fc2 = nn.Linear(num_hidden_units, num_classes)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Properly initialize weights from a gaussian distribution w/ mean=0 and var=1/n\n",
    "        \"\"\"\n",
    "        \n",
    "        def get_conv_fan_in_size(weight):\n",
    "            \"\"\"\n",
    "            Get the fan_in dimension for conv weights.\n",
    "            \"\"\"\n",
    "            size = weight.data.size()\n",
    "            return size[0]*size[1]*size[2]\n",
    "                    \n",
    "        # weights\n",
    "        self.conv1.weight.data.normal_(mean=0, std=np.sqrt(1/get_conv_fan_in_size(self.conv1.weight)))\n",
    "        self.conv2.weight.data.normal_(mean=0, std=np.sqrt(1/get_conv_fan_in_size(self.conv2.weight)))\n",
    "        self.fc1.weight.data.normal_(mean=0, std=np.sqrt(1/len(self.fc1.weight.data[0])))\n",
    "        \n",
    "        # biases\n",
    "        self.conv1.bias.data.normal_(mean=0, std=0.1)\n",
    "        self.conv2.bias.data.normal_(mean=0, std=0.1)\n",
    "        self.fc1.bias.data.normal_(mean=0, std=0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Feed Foward.\n",
    "        \"\"\"\n",
    "        z1 = selu(F.max_pool2d(self.conv1(x), 2))\n",
    "        z2 = selu(F.max_pool2d(self.dropout_conv2(self.conv2(z1)), 2))\n",
    "        z2 = z2.view(-1, 320) # flatten\n",
    "        z3 = selu(self.fc1(z2))\n",
    "        z4 = F.dropout(z3, training=self.training)\n",
    "        z5 = self.fc2(z4)\n",
    "        return [z1, z2, z3, z4, z5], z5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize model components\n",
    "model = selu_cnn(dropout_p, num_hidden_units, num_classes)\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME REMAINING: 10m 32s\n",
      "[EPOCH]: 0, [TRAIN LOSS]: 0.003927, [TRAIN ACC]: 0.909, [TEST LOSS]: 0.001677, [TEST ACC]: 0.971\n",
      "TIME REMAINING: 5m 17s\n",
      "[EPOCH]: 1, [TRAIN LOSS]: 0.003354, [TRAIN ACC]: 0.935, [TEST LOSS]: 0.001636, [TEST ACC]: 0.969\n",
      "TIME REMAINING: 0m 0s\n",
      "[EPOCH]: 2, [TRAIN LOSS]: 0.003347, [TRAIN ACC]: 0.939, [TEST LOSS]: 0.001347, [TEST ACC]: 0.971\n"
     ]
    }
   ],
   "source": [
    "model = train(exp, exp_name, model, criterion, optimizer, train_loader, test_loader, \n",
    "          num_epochs, batch_size, log_interval, learning_rate,\n",
    "          dropout_p, decay_rate, max_grad_norm, cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"figures/key_cnn.png\"></img>\n",
    "<img src=\"figures/activations_cnn.png\"></img>\n",
    "<img src=\"figures/train_acc_cnn.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Alpha dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dropout_p = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create relu experiment\n",
    "exp_name = \"selu_cnn_alpha_dropout\"\n",
    "try:\n",
    "    cc.remove_experiment(exp_name)\n",
    "    exp = cc.create_experiment(exp_name)\n",
    "except:\n",
    "    exp = cc.create_experiment(exp_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ q = \\text{keep_prob, } d = W_{binomial} \\text{ for dropout mask} $$\n",
    "$$ E(x) = \\mu, Var(x) = v $$\n",
    "$$ $$ \n",
    "$$ \\text{w/ dropout with ReLU} (x \\rightarrow -\\infty, y=0) $$\n",
    "$$ \\text{After dropout our activations will not be } xd + 0(1-d) $$\n",
    "$$ \\text{We need to scale this so that we maintain mean=0 and var=1} $$\n",
    "$$ \\text{So, we need to scale the activations (after dropout) by } \\frac{1}{q} $$\n",
    "$$ $$\n",
    "$$ \\text{w/ dropout with SELU} (x \\rightarrow -\\infty, y=-\\lambda\\alpha = \\alpha^{'}) $$\n",
    "$$ \\text{applying dropout here is randomly setting activations to } \\alpha^{'} \\text{ value (instead of zero)} $$\n",
    "$$ \\text{After dropout our activations will not be } xd + \\alpha^{'}(1-d) $$\n",
    "$$ E(xd + \\alpha^{'}(1-d)) =  q\\mu + (1-q)alpha^{'} $$\n",
    "$$ Var(xd + \\alpha^{'}(1-d)) = q((1-q)(\\alpha^{'}-\\mu)^2+v) $$\n",
    "$$ \\text{We need scale these activations using scale factors a and b: } a(xd + \\alpha^{'}(1-d)) + b $$\n",
    "$$ E(a(xd + \\alpha^{'}(1-d)) + b) = \\mu $$\n",
    "$$ Var(a(xd + \\alpha^{'}(1-d)) + b) = v $$\n",
    "$$ \\text{ for } \\mu=0 \\text{ and } v=1: a = (q + {\\alpha^{'}}^{2} q(1-q))^{-0.5}, b = -(q+ {\\alpha^{'}}^{2} q(1-q))^{-0.5}((1-q)\\alpha^{'}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class alpha_dropout(nn.Module):\n",
    "    \"\"\"\n",
    "    Alpha dropout to use during training\n",
    "    in order to scale the activations so\n",
    "    we maintain desired mean and var.\n",
    "    \"\"\"\n",
    "    # -1.7580993408473766 = - scale * alpha = limit of selu as x --> -inf\n",
    "    def __init__(self, dropout_p, alpha_prime=-1.7580993408473766, mean=0, var=1):\n",
    "        \"\"\"\n",
    "        Initialization\n",
    "        \"\"\"\n",
    "        super(alpha_dropout, self).__init__()\n",
    "        self.alpha_prime = alpha_prime\n",
    "        self.q = 1-dropout_p # keep_prob\n",
    "        self.a = 1.0 / np.sqrt( self.q + (self.alpha_prime**2) * self.q * (1-self.q) )\n",
    "        self.b = -1.0 * \\\n",
    "                ( 1.0 / np.sqrt( self.q + self.alpha_prime * self.q * (1-self.q))) * \\\n",
    "                ((1-self.q) * self.alpha_prime)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \"\"\"\n",
    "        if self.q == 1.0 or not self.training:\n",
    "            return x # no dropout\n",
    "        else:\n",
    "            dropout_mask = Variable(torch.Tensor(x.size()).fill_(self.q).bernoulli_())\n",
    "            activations = x.mul(dropout_mask) + self.alpha_prime * (1-dropout_mask)\n",
    "            activations.mul_(self.a).add_(self.b)\n",
    "            return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CNN model\n",
    "class selu_cnn_alpha_dropout(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple CNN for MNIST classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout_p, num_hidden_units, num_classes):\n",
    "        \"\"\"\n",
    "        Initialization.\n",
    "        \"\"\"\n",
    "        super(selu_cnn_alpha_dropout, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.dropout_conv2 = alpha_dropout(0.05)\n",
    "        self.fc1 = nn.Linear(320, num_hidden_units)\n",
    "        self.fc2 = nn.Linear(num_hidden_units, num_classes)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Properly initialize weights from a gaussian distribution w/ mean=0 and var=1/n\n",
    "        \"\"\"\n",
    "        \n",
    "        def get_conv_fan_in_size(weight):\n",
    "            \"\"\"\n",
    "            Get the fan_in dimension for conv weights.\n",
    "            \"\"\"\n",
    "            size = weight.data.size()\n",
    "            return size[0]*size[1]*size[2]\n",
    "                    \n",
    "        # weights\n",
    "        self.conv1.weight.data.normal_(mean=0, std=np.sqrt(1/get_conv_fan_in_size(self.conv1.weight)))\n",
    "        self.conv2.weight.data.normal_(mean=0, std=np.sqrt(1/get_conv_fan_in_size(self.conv2.weight)))\n",
    "        self.fc1.weight.data.normal_(mean=0, std=np.sqrt(1/len(self.fc1.weight.data[0])))\n",
    "        \n",
    "        # biases\n",
    "        self.conv1.bias.data.normal_(mean=0, std=0.1)\n",
    "        self.conv2.bias.data.normal_(mean=0, std=0.1)\n",
    "        self.fc1.bias.data.normal_(mean=0, std=0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Feed Foward.\n",
    "        \"\"\"\n",
    "        z1 = selu(F.max_pool2d(self.conv1(x), 2))\n",
    "        z2 = selu(F.max_pool2d(self.dropout_conv2(self.conv2(z1)), 2))\n",
    "        z2 = z2.view(-1, 320) # flatten\n",
    "        z3 = selu(self.fc1(z2))\n",
    "        z4 = F.dropout(z3, training=self.training)\n",
    "        z5 = self.fc2(z4)\n",
    "        return [z1, z2, z3, z4, z5], z5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize model components\n",
    "model = selu_cnn_alpha_dropout(dropout_p, num_hidden_units, num_classes)\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME REMAINING: 11m 28s\n",
      "[EPOCH]: 0, [TRAIN LOSS]: 0.004308, [TRAIN ACC]: 0.891, [TEST LOSS]: 0.001625, [TEST ACC]: 0.964\n",
      "TIME REMAINING: 5m 41s\n",
      "[EPOCH]: 1, [TRAIN LOSS]: 0.003909, [TRAIN ACC]: 0.916, [TEST LOSS]: 0.001855, [TEST ACC]: 0.973\n",
      "TIME REMAINING: 0m 0s\n",
      "[EPOCH]: 2, [TRAIN LOSS]: 0.003645, [TRAIN ACC]: 0.927, [TEST LOSS]: 0.001292, [TEST ACC]: 0.976\n"
     ]
    }
   ],
   "source": [
    "model = train(exp, exp_name, model, criterion, optimizer, train_loader, test_loader, \n",
    "          num_epochs, batch_size, log_interval, learning_rate,\n",
    "          dropout_p, decay_rate, max_grad_norm, cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/key_dropout.png\"></img>\n",
    "<img src=\"figures/activations_dropout.png\"></img>\n",
    "<img src=\"figures/activations_dropout_2.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Resources (not my code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to know the scaling and alpha values for any arbitrary fixed point mean and variance, use this code by the authors. Credit: https://github.com/bioinf-jku/SNNs/blob/master/getSELUparameters.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import erf,erfc\n",
    "from sympy import Symbol, solve, nsolve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSeluParameters(fixedpointMean=0, fixedpointVar=1):\n",
    "    \"\"\" \n",
    "    Finding the parameters of the \n",
    "    SELU activation function. The \n",
    "    function returns alpha and lambda \n",
    "    for the desired fixed point. \n",
    "    \"\"\"\n",
    "    \n",
    "    import sympy\n",
    "    from sympy import Symbol, solve, nsolve\n",
    "\n",
    "    aa = Symbol('aa')\n",
    "    ll = Symbol('ll')\n",
    "    nu = fixedpointMean \n",
    "    tau = fixedpointVar \n",
    "\n",
    "    mean =  0.5*ll*(nu + np.exp(-nu**2/(2*tau))*np.sqrt(2/np.pi)*np.sqrt(tau) + \\\n",
    "                        nu*erf(nu/(np.sqrt(2*tau))) - aa*erfc(nu/(np.sqrt(2*tau))) + \\\n",
    "                        np.exp(nu+tau/2)*aa*erfc((nu+tau)/(np.sqrt(2*tau))))\n",
    "\n",
    "    var = 0.5*ll**2*(np.exp(-nu**2/(2*tau))*np.sqrt(2/np.pi*tau)*nu + (nu**2+tau)* \\\n",
    "                          (1+erf(nu/(np.sqrt(2*tau)))) + aa**2 *erfc(nu/(np.sqrt(2*tau))) \\\n",
    "                          - aa**2 * 2 *np.exp(nu+tau/2)*erfc((nu+tau)/(np.sqrt(2*tau)))+ \\\n",
    "                          aa**2*np.exp(2*(nu+tau))*erfc((nu+2*tau)/(np.sqrt(2*tau))) ) - mean**2\n",
    "\n",
    "    eq1 = mean - nu\n",
    "    eq2 = var - tau\n",
    "\n",
    "    res = nsolve( (eq2, eq1), (aa,ll), (1.67,1.05))\n",
    "    return float(res[0]),float(res[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.6732632423543774, 1.0507009873554802)\n",
      "(1.9622211815386628, 1.0498194717277889)\n"
     ]
    }
   ],
   "source": [
    "print (getSeluParameters(fixedpointMean=0, fixedpointVar=1))\n",
    "print (getSeluParameters(fixedpointMean=0.1, fixedpointVar=2.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
